{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Basics for Beginners\n",
    "\n",
    "### 1. Introduction to Neural Networks\n",
    "Before diving into the more complex concepts, understand what a neural network is. At its core, a neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\n",
    "\n",
    "**Resource:** For a fundamental understanding, read [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen\n",
    "\n",
    "### 2. Activation Functions\n",
    "Activation functions are crucial, they define the ouput of a node(or neuron) given a set of inputs. Essentially, they decide whether a neuron should be activated or not.\n",
    "\n",
    "Common Activation Functions:\n",
    "\n",
    "* Sigmoid: A function that maps any value to a value between 0 and 1. It's useful for binary classification.\n",
    "* ReLU (Rectified Linear Unit): Allows only positive values to pass through and blocks negative values. With this activation function, all negative inputs automatically becomes 0 as an output.\n",
    "* Leaky ReLU: Allows positive values and negative values. However, with this activation function, the negative inputs automatically becomes 0.1 * the negative input as the output.\n",
    "* Softmax: Often used in the final layer of a neural network-based classifier. It's useful for multi-class classification.\n",
    "\n",
    "**Implementation of the Sigmoid activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `import math`: This line of code tells Python to import a module called \"math,\" which provides mathematical functions and constants.\n",
    "\n",
    "* `def sigmoid(x):`: This line defines a function named sigmoid that takes one argument called `x`. In Python, you create functions using the def keyword followed by the function name and a pair of parentheses that can contain input parameters.\n",
    "\n",
    "* `return 1/(1 + math.exp(-x))`: This line is the body of the sigmoid function and contains the code that computes the sigmoid function's value for the given input `x`.\n",
    "\n",
    "* `math.exp(-x)`: Calculates the exponential of the negative value of `x`. The `math.exp()` function calculates `e` raised to the power of `-x`, where `e` is approximately 2.71828.\n",
    "\n",
    "* `1 + math.exp(-x)`: Adds 1 to the result of `math.exp(-x)`. It increases the value obtained in the previous step by 1.\n",
    "\n",
    "* `1 / (1 + math.exp(-x))`: Calculates 1 divided by the result from the previous step. This division results in the sigmoid function's value for the input `x`. The sigmoid function maps any real number `x` to a value between 0 and 1, commonly used in machine learning and neural networks for binary classification tasks.\n",
    "\n",
    "So, when you call `sigmoid(x)` with a specific value for `x`, it will return the sigmoid of that value. For example, `sigmoid(0)` would return approximately 0.5, and `sigmoid(1)` would return approximately 0.73105."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the tanh activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(-56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `import math`: This line of code imports the \"math\" module, which provides various mathematical functions and constants for our use.\n",
    "\n",
    "* `def tanh(x):`: This line defines a function named `tanh` that takes one argument `x`. In Python, you define functions using the `def` keyword, followed by the function name and a pair of parentheses that may contain input parameters.\n",
    "\n",
    "* `return (math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))`: This line is the body of the `tanh` function and contains the code that calculates the hyperbolic tangent (tanh) of the given input `x`. Let's break down this expression:\n",
    "\n",
    "- `math.exp(x)`: Calculates the exponential of `x`. The `math.exp()` function computes `e` raised to the power of `x`, where `e` is approximately 2.71828.\n",
    "\n",
    "- `math.exp(-x)`: Calculates the exponential of the negative value of `x`, which is `e` raised to the power of `-x`.\n",
    "\n",
    "- `(math.exp(x) - math.exp(-x))`: Calculates the difference between `math.exp(x)` and `math.exp(-x)`.\n",
    "\n",
    "- `(math.exp(x) + math.exp(-x))`: Calculates the sum of `math.exp(x)` and `math.exp(-x)`.\n",
    "\n",
    "The final result is `(math.exp(x) - math.exp(-x))/(math.exp(x) + math.exp(-x))`, which represents the hyperbolic tangent (`tanh`) of `x`.\n",
    "\n",
    "The hyperbolic tangent function, `tanh(x)`, maps any real number `x` to a value between -1 and 1. It is a common mathematical function used in various applications, including neural networks and signal processing.\n",
    "\n",
    "For example, tanh(0) would return 0.0, and tanh(1) would return approximately 0.76159.\n",
    "\n",
    "\n",
    "**Implementation of the ReLU activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `def relu(x):`: This line defines a function named `relu` that takes one argument `x`. In Python, functions are defined using the def keyword, followed by the function name and a pair of parentheses that may contain input parameters.\n",
    "\n",
    "* `return max(0, x)`: This line is the body of the `relu` function and contains the code that calculates the Rectified Linear Unit (ReLU) of the given input `x`.\n",
    "\n",
    "* `max(0, x)`: The `max()` function in Python takes two arguments and returns the maximum of the two values. In this case, it takes 0 and x as arguments.\n",
    "\n",
    "If x is greater than or equal to 0, `max(0, x)` will return `x` because `x` is the larger of the two values.\n",
    "\n",
    "If `x` is less than 0, `max(0, x)` will return 0 because 0 is the larger of the two values.\n",
    "\n",
    "The ReLU function, `relu(x)`, is commonly used in neural networks. It essentially replaces negative values in x with 0 and leaves positive values unchanged.\n",
    "\n",
    "For example, `relu(3)` would return 3, `relu(-2)` would return 0, and `relu(0)` would return 0.\n",
    "\n",
    "This function helps introduce non-linearity into neural networks, making them capable of learning complex patterns and representations in data.\n",
    "\n",
    "\n",
    "**Implementation of the Leaky ReLU activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(0.1 * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `def leaky_relu(x):`: This line defines a function named `leaky_relu` that takes one argument x. In Python, functions are defined using the def keyword, followed by the function name and a pair of parentheses that may contain input parameters.\n",
    "\n",
    "* `return max(0.1 * x, x)`: This line is the body of the `leaky_relu` function and contains the code that calculates the Leaky Rectified Linear Unit (Leaky ReLU) of the given input `x`.\n",
    "\n",
    "* `0.1 * x`: This expression calculates 0.1 times x, resulting in `0.1x`. This is the \"leak\" component of the Leaky ReLU.\n",
    "\n",
    "* `max(0.1 * x, x)`: The `max()` function in Python takes two arguments and returns the maximum of the two values. In this case, it takes `0.1 * x` and `x` as arguments.\n",
    "\n",
    "If `x` is greater than or equal to 0, `max(0.1 * x, x)` will return `x` because `x` is the larger of the two values.\n",
    "\n",
    "If `x` is less than 0, `max(0.1 * x, x)` will return `0.1 * x` because `0.1 * x` is the larger of the two values, and this introduces a small \"leak\" for negative inputs.\n",
    "\n",
    "The Leaky ReLU function, `leaky_relu(x)`, is another activation function used in neural networks and machine learning. It's similar to the standard ReLU but allows a small gradient for negative inputs, preventing the \"dying ReLU\" problem.\n",
    "\n",
    "For example, `leaky_relu(3)` would return 3, `leaky_relu(-2)` would return -0.2, and `leaky_relu(0)` would return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss Functions\n",
    "A loss function measures how well the neutral network is performing. It's a method of evaluating how well specific algorithm models the given data.\n",
    "\n",
    "Common Loss Functions:\n",
    "* Mean Squared Error, Mean Absolute Error and other error metrics: Commonly used for regression tasks.\n",
    "* Cross-Entropy: Oftern used in classification tasks.\n",
    "* Binary Cross-Entropy: Used for binary classification tasks.\n",
    "\n",
    "\n",
    "**Implementation of the Mean Absolute Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_predicted = np.array([1,1,0,0,1])\n",
    "y_true = np.array([0.7,0.3,1,0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_predicted):\n",
    "    return np.mean(np.abs(y_true - y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_true, y_predicted)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `import numpy as np` imports the NumPy library as `np`.\n",
    "\n",
    "* `mean_absolute_error` is a function that takes two NumPy arrays `y_true` and `y_predicted` as input.\n",
    "\n",
    "* `np.abs(y_true - y_predicted)` calculates the absolute differences between the elements of `y_true` and `y_predicted`.\n",
    "\n",
    "* `np.mean()` computes the mean (average) of these absolute differences, giving you the Mean Absolute Error.\n",
    "\n",
    "You can use this function to calculate the MAE for any pair of arrays `y_true` and `y_predicted` as shown in the example usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the Mean Squared Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_predicted):\n",
    "    return np.mean((y_true - y_predicted)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.366\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_true, y_predicted)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `mean_squared_error` is a function that takes two `NumPy` arrays `y_true` and `y_predicted` as input.\n",
    "\n",
    "* `(y_true - y_predicted)**2` calculates the squared differences between the elements of `y_true` and `y_predicted`.\n",
    "\n",
    "* `np.mean()` computes the mean (average) of these squared differences, giving you the Mean Squared Error.\n",
    "\n",
    "You can use this function to calculate the MSE for any pair of arrays `y_true` and `y_predicted` as shown in the example usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation of the Log Cross-Entropy** - For multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_3600\\262464415.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log([0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-inf])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cross_entropy(y_true, y_predicted):\n",
    "    return -np.sum(y_true * np.log(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Cross-Entropy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_3600\\3331548531.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y_true * np.log(y_predicted))\n",
      "C:\\Users\\Windows\\AppData\\Local\\Temp\\ipykernel_3600\\3331548531.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(y_true * np.log(y_predicted))\n"
     ]
    }
   ],
   "source": [
    "log_ce = log_cross_entropy(y_true, y_predicted)\n",
    "print(\"Log Cross-Entropy:\", log_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Derivatives and Gradients\n",
    "Understanding derivatives is crucial in neural networks as they are fundamental to the process of learning, specifically in optimizing the loss function using methods like gradient descent.\n",
    "\n",
    "* Derivative: Measures how a function changes as its input changes. In machine learning, it's used to find the rate of change of the loss function with respect to the weights.\n",
    "* Gradient: It's a vector that contains all the partial derivatives of a function.\n",
    "\n",
    "To get more understanding on Derivatives, you might want to check out [Khan Academy's Introduction to Derivatives](https://www.khanacademy.org/math/old-differential-calculus/derivative-intro-dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
