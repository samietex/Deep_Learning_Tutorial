{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset for Neural Network\n",
    "\n",
    "Welcome to this beginner's tutorial on neural networks using the MNIST dataset! This notebook is designed as an introductory guide to understand and implement neural networks for digit recognition. The MNIST dataset, consisting of 28x28 grayscale images of handwritten digits, serves as an ideal starting point due to its simplicity and the extensive community support around it. Throughout this tutorial, we will cover the basics of setting up your environment, preprocessing the data, constructing a simple neural network model, training this model, and evaluating its performance. By the end of this notebook, you'll have a foundational understanding of neural networks and hands-on experience building one with a classic dataset. Let's dive into the exciting world of neural networks!\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "We will be setting up the environment. Most neural network projects are implemented using Python. For beginners with no prior knowledge or experience in Python, you might want to check out [this]() where I curated an introduction to Neural Network Basics.\n",
    "\n",
    "You will also need to install several packages like NumPy, Matplotlib, Keras, and TensorFlow or PyTorch. This can be done in the Anaconda Prompt by using pip or conda to install the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy matplotlib tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully installed the packages, you can then go ahead with this tutorial.\n",
    "\n",
    "### Step 1: Importing all necessary libraries\n",
    "The next step after installing all libraries is to import them. It is adviseable to restart your kernel before importing just installed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is a highly popular library used in Python for data manipulation and analysis. It offers data structures and operations for manipulating numerical tables and time series. You can check the Panda user guide [here](https://pandas.pydata.org/docs/user_guide/index.html) for more information.\n",
    "\n",
    "**NumPy** stands for Numerical Python and it's a foundational package for numerical computing in Python. It provides support for arrays (including multidimensional arrays). matrices. and a large collection of high-level mathematical fuctions to operate on these structures. Here'a [documentation](https://numpy.org/doc/stable/user/whatisnumpy.html) that will introduce you to NumPy\n",
    "\n",
    "**Matplotlib** is a plotting library in Python. It's widely used for creating static, interactive, and animated visualizations in Python. `pyplot` is a module in Matplotlub that provides a MATLAB-like interface for making plots and charts. Here'a [documentation](https://matplotlib.org/stable/index.html) that will introduce you to Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**: This dataset is a collection of handwritten digits (0 to 9) commonly used for training and testing machine learning models.\n",
    "\n",
    "`(x_train, y_train), (x_test, y_test)`: This part of the code is using Python's tuple unpacking. The dataset is divided into two sets: training data and testing data.\n",
    "\n",
    "`x_train` contains the images that the model will use to learn patterns or features.\n",
    "`y_train` contains the corresponding labels (the actual numbers the images represent) for the training images.\n",
    "`x_test` is a separate set of images used to evaluate how well the model performs after it has been trained.\n",
    "`y_test` contains the corresponding labels for the testing images.\n",
    "`keras.datasets.mnist.load_data()`: This function is from the Keras library and specifically loads the MNIST dataset. It fetches the dataset and divides it into the training and testing sets, then assigns the images and labels to X_train, y_train, X_test, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`x_train[0].shape` refers to the shape of the first image in the training dataset `(x_train)`. In machine learning, especially when dealing with image data, the shape usually represents the dimensions of the data.\n",
    "\n",
    "For the MNIST dataset, `x_train[0]` is the first image in the training set. Calling .shape on it gives you information about the image's dimensions, typically in the form of (height, width).\n",
    "\n",
    "In this case, it signifies the shape of the image. For MNIST, the images are grayscale and have a shape of (28, 28), meaning each image is 28 pixels in height and 28 pixels in width.\n",
    "\n",
    "So, when you call `x_train[0].shape`, you're essentially checking the dimensions of the first image in the MNIST training set, which is a 28x28 grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`x_train[0]` represents the first image in the training set of the MNIST dataset. This specific entry in `x_train` contains the pixel values of the image.\n",
    "\n",
    "For the MNIST dataset, `x_train[0]` is a 28x28 array representing a grayscale image. Each element in this array corresponds to the grayscale intensity of a pixel in the image. The values typically range from 0 to 255, where 0 is black and 255 is white.\n",
    "\n",
    "If you were to print `x_train[0]`, you would see a 28x28 array of numbers, where each number indicates the intensity of the corresponding pixel in the image. This array of pixel values forms the representation of the handwritten digit that the machine learning model will learn from during its training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a13a950410>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plt.matshow(x_train[0])` is a command using Matplotlib to visualize the first image in the dataset `x_train` as a matrix.\n",
    "\n",
    "`matshow()` is a function from the `pyplot` module specifically designed to display a matrix of 2D array as a color-coded image. It's particularly useful for visualizing the MNIST dataset.\n",
    "\n",
    "`x_train[0]` indexes into the `x_train` array to retrieve the first image. In the MNIST dataset, `x_train` is an array where each element is a 28x28 array representing a grayscale image of a handwritten digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`y_train[0]` refers to the label associated with the first image in the training set of the MNIST dataset. In the MNIST dataset, each image `(like x_train[0])` has a corresponding label that identifies the digit represented by that image.\n",
    "\n",
    "So, `y_train[0]` contains the label for the first image in the training set, specifying the actual digit written in the image represented by `x_train[0]`.\n",
    "\n",
    "For instance, if `y_train[0]` is 5, it means that the handwritten digit in `x_train[0]` is the number 5. The labels in the MNIST dataset range from 0 to 9, representing the digits from 0 to 9 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is performing a common preprocessing step called normalization on the pixel values of the images in the MNIST dataset. In this case, it's scaling the pixel values to a range between 0 and 1.\n",
    "\n",
    "For images represented as pixel values ranging from 0 to 255 (in the case of grayscale images), dividing each pixel value by 255 effectively scales them down to a range between 0 and 1. This normalization process can be beneficial for training machine learning models for several reasons:\n",
    "\n",
    "1. Faster convergence: Normalizing the input data to a smaller range often helps the optimization algorithm converge faster during training.\n",
    "\n",
    "2. Stability in training: Normalization helps in preventing potential issues with exploding or vanishing gradients that could occur when using larger pixel value ranges.\n",
    "\n",
    "The lines `x_train = x_train / 255 and x_test = x_test / 255` are dividing every pixel value in the training and testing sets, respectively, by 255, ensuring all pixel values are between 0 and 1.\n",
    "\n",
    "This preprocessing step does not alter the relative differences between pixel values but adjusts their scale to a range that is more suitable for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flattened = x_train.reshape(len(x_train), 28 * 28)\n",
    "x_test_flattened = x_test.reshape(len(x_test), 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is reshaping the 28x28 images in the MNIST dataset into flattened arrays of length 784 (28 * 28). It's transforming the 2D array representation of each image into a 1D array while maintaining all the pixel information.\n",
    "\n",
    "Here's what's happening in these lines:\n",
    "\n",
    "1. `x_train_flattened = x_train.reshape(len(x_train), 28 * 28)`: This line takes the `x_train` dataset, which contains 2D arrays representing images (28x28 pixels), and reshapes each image into a flattened 1D array with a length of 784 (28*28). The `reshape()` function is used to change the shape of the array, where `len(x_train)` represents the number of images in the training set.\n",
    "\n",
    "2. `x_test_flattened = x_test.reshape(len(x_test), 28 * 28)`: Similarly, this line does the same for the testing dataset `(x_test)`, flattening each image into a 1D array of length 784.\n",
    "\n",
    "This transformation from a 2D representation (a matrix) to a 1D representation (a vector) is often done as a preprocessing step before feeding the data into certain types of machine learning models, especially those that expect a flat input format rather than a structured grid (such as fully connected neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_flattened[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with data preparing and preprocessing, we can now build our neural network model using Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.4675 - accuracy: 0.8789\n",
      "Epoch 2/200\n",
      "1875/1875 [==============================] - 1s 758us/step - loss: 0.3038 - accuracy: 0.9157\n",
      "Epoch 3/200\n",
      "1875/1875 [==============================] - 1s 751us/step - loss: 0.2833 - accuracy: 0.9209\n",
      "Epoch 4/200\n",
      "1875/1875 [==============================] - 1s 798us/step - loss: 0.2735 - accuracy: 0.9234\n",
      "Epoch 5/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2656 - accuracy: 0.9259\n",
      "Epoch 6/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2619 - accuracy: 0.9272\n",
      "Epoch 7/200\n",
      "1875/1875 [==============================] - 2s 868us/step - loss: 0.2582 - accuracy: 0.9281\n",
      "Epoch 8/200\n",
      "1875/1875 [==============================] - 2s 830us/step - loss: 0.2553 - accuracy: 0.9296\n",
      "Epoch 9/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2531 - accuracy: 0.9299\n",
      "Epoch 10/200\n",
      "1875/1875 [==============================] - 2s 836us/step - loss: 0.2509 - accuracy: 0.9310\n",
      "Epoch 11/200\n",
      "1875/1875 [==============================] - 2s 979us/step - loss: 0.2490 - accuracy: 0.9307\n",
      "Epoch 12/200\n",
      "1875/1875 [==============================] - 2s 823us/step - loss: 0.2476 - accuracy: 0.9318\n",
      "Epoch 13/200\n",
      "1875/1875 [==============================] - 1s 774us/step - loss: 0.2462 - accuracy: 0.9319\n",
      "Epoch 14/200\n",
      "1875/1875 [==============================] - 1s 747us/step - loss: 0.2448 - accuracy: 0.9323\n",
      "Epoch 15/200\n",
      "1875/1875 [==============================] - 2s 810us/step - loss: 0.2443 - accuracy: 0.9331\n",
      "Epoch 16/200\n",
      "1875/1875 [==============================] - 1s 738us/step - loss: 0.2430 - accuracy: 0.9326\n",
      "Epoch 17/200\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2420 - accuracy: 0.9337\n",
      "Epoch 18/200\n",
      "1875/1875 [==============================] - 2s 815us/step - loss: 0.2414 - accuracy: 0.9337\n",
      "Epoch 19/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2405 - accuracy: 0.9337\n",
      "Epoch 20/200\n",
      "1875/1875 [==============================] - 2s 809us/step - loss: 0.2400 - accuracy: 0.9339\n",
      "Epoch 21/200\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.2392 - accuracy: 0.9342\n",
      "Epoch 22/200\n",
      "1875/1875 [==============================] - 2s 931us/step - loss: 0.2384 - accuracy: 0.9344\n",
      "Epoch 23/200\n",
      "1875/1875 [==============================] - 1s 757us/step - loss: 0.2377 - accuracy: 0.9348\n",
      "Epoch 24/200\n",
      "1875/1875 [==============================] - 2s 941us/step - loss: 0.2371 - accuracy: 0.9347\n",
      "Epoch 25/200\n",
      "1875/1875 [==============================] - 2s 934us/step - loss: 0.2369 - accuracy: 0.9349\n",
      "Epoch 26/200\n",
      "1875/1875 [==============================] - 1s 789us/step - loss: 0.2359 - accuracy: 0.9344\n",
      "Epoch 27/200\n",
      "1875/1875 [==============================] - 2s 952us/step - loss: 0.2356 - accuracy: 0.9348\n",
      "Epoch 28/200\n",
      "1875/1875 [==============================] - 2s 907us/step - loss: 0.2354 - accuracy: 0.9356\n",
      "Epoch 29/200\n",
      "1875/1875 [==============================] - 2s 861us/step - loss: 0.2348 - accuracy: 0.9352\n",
      "Epoch 30/200\n",
      "1875/1875 [==============================] - 1s 770us/step - loss: 0.2343 - accuracy: 0.9356\n",
      "Epoch 31/200\n",
      "1875/1875 [==============================] - 1s 711us/step - loss: 0.2342 - accuracy: 0.9353\n",
      "Epoch 32/200\n",
      "1875/1875 [==============================] - 2s 824us/step - loss: 0.2332 - accuracy: 0.9349\n",
      "Epoch 33/200\n",
      "1875/1875 [==============================] - 1s 733us/step - loss: 0.2331 - accuracy: 0.9351\n",
      "Epoch 34/200\n",
      "1875/1875 [==============================] - 1s 709us/step - loss: 0.2329 - accuracy: 0.9357\n",
      "Epoch 35/200\n",
      "1875/1875 [==============================] - 1s 744us/step - loss: 0.2323 - accuracy: 0.9354\n",
      "Epoch 36/200\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2322 - accuracy: 0.9365\n",
      "Epoch 37/200\n",
      "1875/1875 [==============================] - 2s 854us/step - loss: 0.2316 - accuracy: 0.9355\n",
      "Epoch 38/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2315 - accuracy: 0.9349\n",
      "Epoch 39/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2314 - accuracy: 0.9361\n",
      "Epoch 40/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2308 - accuracy: 0.9360\n",
      "Epoch 41/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2308 - accuracy: 0.9363\n",
      "Epoch 42/200\n",
      "1875/1875 [==============================] - 2s 881us/step - loss: 0.2309 - accuracy: 0.9362\n",
      "Epoch 43/200\n",
      "1875/1875 [==============================] - 1s 738us/step - loss: 0.2299 - accuracy: 0.9367\n",
      "Epoch 44/200\n",
      "1875/1875 [==============================] - 1s 761us/step - loss: 0.2295 - accuracy: 0.9359\n",
      "Epoch 45/200\n",
      "1875/1875 [==============================] - 2s 966us/step - loss: 0.2295 - accuracy: 0.9367\n",
      "Epoch 46/200\n",
      "1875/1875 [==============================] - 2s 954us/step - loss: 0.2294 - accuracy: 0.9362\n",
      "Epoch 47/200\n",
      "1875/1875 [==============================] - 2s 845us/step - loss: 0.2293 - accuracy: 0.9365\n",
      "Epoch 48/200\n",
      "1875/1875 [==============================] - 2s 913us/step - loss: 0.2290 - accuracy: 0.9364\n",
      "Epoch 49/200\n",
      "1875/1875 [==============================] - 2s 848us/step - loss: 0.2288 - accuracy: 0.9367\n",
      "Epoch 50/200\n",
      "1875/1875 [==============================] - 2s 851us/step - loss: 0.2287 - accuracy: 0.9363\n",
      "Epoch 51/200\n",
      "1875/1875 [==============================] - 1s 776us/step - loss: 0.2284 - accuracy: 0.9368\n",
      "Epoch 52/200\n",
      "1875/1875 [==============================] - 2s 864us/step - loss: 0.2283 - accuracy: 0.9362\n",
      "Epoch 53/200\n",
      "1875/1875 [==============================] - 2s 809us/step - loss: 0.2275 - accuracy: 0.9367\n",
      "Epoch 54/200\n",
      "1875/1875 [==============================] - 1s 732us/step - loss: 0.2277 - accuracy: 0.9369\n",
      "Epoch 55/200\n",
      "1875/1875 [==============================] - 1s 748us/step - loss: 0.2270 - accuracy: 0.9368\n",
      "Epoch 56/200\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2273 - accuracy: 0.9368\n",
      "Epoch 57/200\n",
      "1875/1875 [==============================] - 1s 799us/step - loss: 0.2275 - accuracy: 0.9371\n",
      "Epoch 58/200\n",
      "1875/1875 [==============================] - 1s 781us/step - loss: 0.2267 - accuracy: 0.9368\n",
      "Epoch 59/200\n",
      "1875/1875 [==============================] - 1s 745us/step - loss: 0.2267 - accuracy: 0.9367\n",
      "Epoch 60/200\n",
      "1875/1875 [==============================] - 1s 720us/step - loss: 0.2264 - accuracy: 0.9374\n",
      "Epoch 61/200\n",
      "1875/1875 [==============================] - 1s 759us/step - loss: 0.2265 - accuracy: 0.9372\n",
      "Epoch 62/200\n",
      "1875/1875 [==============================] - 2s 925us/step - loss: 0.2266 - accuracy: 0.9372\n",
      "Epoch 63/200\n",
      "1875/1875 [==============================] - 1s 766us/step - loss: 0.2258 - accuracy: 0.9373\n",
      "Epoch 64/200\n",
      "1875/1875 [==============================] - 1s 799us/step - loss: 0.2256 - accuracy: 0.9373\n",
      "Epoch 65/200\n",
      "1875/1875 [==============================] - 1s 782us/step - loss: 0.2257 - accuracy: 0.9375\n",
      "Epoch 66/200\n",
      "1875/1875 [==============================] - 1s 767us/step - loss: 0.2254 - accuracy: 0.9372\n",
      "Epoch 67/200\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2256 - accuracy: 0.9372\n",
      "Epoch 68/200\n",
      "1875/1875 [==============================] - 1s 728us/step - loss: 0.2249 - accuracy: 0.9383\n",
      "Epoch 69/200\n",
      "1875/1875 [==============================] - 1s 748us/step - loss: 0.2255 - accuracy: 0.9374\n",
      "Epoch 70/200\n",
      "1875/1875 [==============================] - 1s 757us/step - loss: 0.2250 - accuracy: 0.9376\n",
      "Epoch 71/200\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2249 - accuracy: 0.9375\n",
      "Epoch 72/200\n",
      "1875/1875 [==============================] - 2s 817us/step - loss: 0.2249 - accuracy: 0.9373\n",
      "Epoch 73/200\n",
      "1875/1875 [==============================] - 1s 791us/step - loss: 0.2245 - accuracy: 0.9380\n",
      "Epoch 74/200\n",
      "1875/1875 [==============================] - 1s 736us/step - loss: 0.2245 - accuracy: 0.9376\n",
      "Epoch 75/200\n",
      "1875/1875 [==============================] - 2s 920us/step - loss: 0.2247 - accuracy: 0.9372\n",
      "Epoch 76/200\n",
      "1875/1875 [==============================] - 2s 824us/step - loss: 0.2243 - accuracy: 0.9376\n",
      "Epoch 77/200\n",
      "1875/1875 [==============================] - 1s 757us/step - loss: 0.2240 - accuracy: 0.9371\n",
      "Epoch 78/200\n",
      "1875/1875 [==============================] - 2s 801us/step - loss: 0.2238 - accuracy: 0.9375\n",
      "Epoch 79/200\n",
      "1875/1875 [==============================] - 2s 952us/step - loss: 0.2236 - accuracy: 0.9381\n",
      "Epoch 80/200\n",
      "1875/1875 [==============================] - 2s 901us/step - loss: 0.2241 - accuracy: 0.9381\n",
      "Epoch 81/200\n",
      "1875/1875 [==============================] - 2s 900us/step - loss: 0.2233 - accuracy: 0.9373\n",
      "Epoch 82/200\n",
      "1875/1875 [==============================] - 2s 844us/step - loss: 0.2232 - accuracy: 0.9382\n",
      "Epoch 83/200\n",
      "1875/1875 [==============================] - 2s 803us/step - loss: 0.2237 - accuracy: 0.9374\n",
      "Epoch 84/200\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2232 - accuracy: 0.9375\n",
      "Epoch 85/200\n",
      "1875/1875 [==============================] - 1s 792us/step - loss: 0.2236 - accuracy: 0.9370\n",
      "Epoch 86/200\n",
      "1875/1875 [==============================] - 1s 774us/step - loss: 0.2232 - accuracy: 0.9378\n",
      "Epoch 87/200\n",
      "1875/1875 [==============================] - 1s 791us/step - loss: 0.2235 - accuracy: 0.9377\n",
      "Epoch 88/200\n",
      "1875/1875 [==============================] - 1s 781us/step - loss: 0.2229 - accuracy: 0.9381\n",
      "Epoch 89/200\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2228 - accuracy: 0.9380\n",
      "Epoch 90/200\n",
      "1875/1875 [==============================] - 2s 807us/step - loss: 0.2225 - accuracy: 0.9388\n",
      "Epoch 91/200\n",
      "1875/1875 [==============================] - 2s 801us/step - loss: 0.2226 - accuracy: 0.9379\n",
      "Epoch 92/200\n",
      "1875/1875 [==============================] - 2s 804us/step - loss: 0.2225 - accuracy: 0.9376\n",
      "Epoch 93/200\n",
      "1875/1875 [==============================] - 1s 774us/step - loss: 0.2226 - accuracy: 0.9380\n",
      "Epoch 94/200\n",
      "1875/1875 [==============================] - 1s 782us/step - loss: 0.2226 - accuracy: 0.9379\n",
      "Epoch 95/200\n",
      "1875/1875 [==============================] - 1s 793us/step - loss: 0.2221 - accuracy: 0.9383\n",
      "Epoch 96/200\n",
      "1875/1875 [==============================] - 1s 773us/step - loss: 0.2221 - accuracy: 0.9379\n",
      "Epoch 97/200\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2223 - accuracy: 0.9384\n",
      "Epoch 98/200\n",
      "1875/1875 [==============================] - 1s 760us/step - loss: 0.2220 - accuracy: 0.9382\n",
      "Epoch 99/200\n",
      "1875/1875 [==============================] - 1s 765us/step - loss: 0.2221 - accuracy: 0.9380\n",
      "Epoch 100/200\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.2221 - accuracy: 0.9384\n",
      "Epoch 101/200\n",
      "1875/1875 [==============================] - 1s 754us/step - loss: 0.2219 - accuracy: 0.9384\n",
      "Epoch 102/200\n",
      "1875/1875 [==============================] - 1s 780us/step - loss: 0.2221 - accuracy: 0.9379\n",
      "Epoch 103/200\n",
      "1875/1875 [==============================] - 1s 775us/step - loss: 0.2216 - accuracy: 0.9384\n",
      "Epoch 104/200\n",
      "1875/1875 [==============================] - 1s 763us/step - loss: 0.2215 - accuracy: 0.9382\n",
      "Epoch 105/200\n",
      "1875/1875 [==============================] - 2s 802us/step - loss: 0.2218 - accuracy: 0.9384\n",
      "Epoch 106/200\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2215 - accuracy: 0.9381\n",
      "Epoch 107/200\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2212 - accuracy: 0.9379\n",
      "Epoch 108/200\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2215 - accuracy: 0.9376\n",
      "Epoch 109/200\n",
      "1875/1875 [==============================] - 1s 794us/step - loss: 0.2208 - accuracy: 0.9385\n",
      "Epoch 110/200\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2210 - accuracy: 0.9389\n",
      "Epoch 111/200\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2211 - accuracy: 0.9384\n",
      "Epoch 112/200\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2207 - accuracy: 0.9389\n",
      "Epoch 113/200\n",
      "1875/1875 [==============================] - 1s 774us/step - loss: 0.2205 - accuracy: 0.9382\n",
      "Epoch 114/200\n",
      "1875/1875 [==============================] - 1s 770us/step - loss: 0.2211 - accuracy: 0.9385\n",
      "Epoch 115/200\n",
      "1875/1875 [==============================] - 2s 805us/step - loss: 0.2210 - accuracy: 0.9382\n",
      "Epoch 116/200\n",
      "1875/1875 [==============================] - 1s 797us/step - loss: 0.2209 - accuracy: 0.9383\n",
      "Epoch 117/200\n",
      "1875/1875 [==============================] - 1s 773us/step - loss: 0.2207 - accuracy: 0.9383\n",
      "Epoch 118/200\n",
      "1875/1875 [==============================] - 1s 768us/step - loss: 0.2204 - accuracy: 0.9384\n",
      "Epoch 119/200\n",
      "1875/1875 [==============================] - 1s 768us/step - loss: 0.2201 - accuracy: 0.9385\n",
      "Epoch 120/200\n",
      "1875/1875 [==============================] - 1s 763us/step - loss: 0.2204 - accuracy: 0.9391\n",
      "Epoch 121/200\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.2205 - accuracy: 0.9378\n",
      "Epoch 122/200\n",
      "1875/1875 [==============================] - 1s 800us/step - loss: 0.2202 - accuracy: 0.9384\n",
      "Epoch 123/200\n",
      "1875/1875 [==============================] - 1s 786us/step - loss: 0.2201 - accuracy: 0.9389\n",
      "Epoch 124/200\n",
      "1875/1875 [==============================] - 1s 787us/step - loss: 0.2202 - accuracy: 0.9385\n",
      "Epoch 125/200\n",
      "1875/1875 [==============================] - 1s 767us/step - loss: 0.2199 - accuracy: 0.9386\n",
      "Epoch 126/200\n",
      "1875/1875 [==============================] - 1s 764us/step - loss: 0.2199 - accuracy: 0.9390\n",
      "Epoch 127/200\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.2202 - accuracy: 0.9387\n",
      "Epoch 128/200\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2201 - accuracy: 0.9390\n",
      "Epoch 129/200\n",
      "1875/1875 [==============================] - 1s 772us/step - loss: 0.2195 - accuracy: 0.9385\n",
      "Epoch 130/200\n",
      "1875/1875 [==============================] - 1s 761us/step - loss: 0.2196 - accuracy: 0.9384\n",
      "Epoch 131/200\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2198 - accuracy: 0.9390\n",
      "Epoch 132/200\n",
      "1875/1875 [==============================] - 1s 776us/step - loss: 0.2197 - accuracy: 0.9387\n",
      "Epoch 133/200\n",
      "1875/1875 [==============================] - 1s 785us/step - loss: 0.2193 - accuracy: 0.9389\n",
      "Epoch 134/200\n",
      "1875/1875 [==============================] - 1s 779us/step - loss: 0.2195 - accuracy: 0.9388\n",
      "Epoch 135/200\n",
      "1875/1875 [==============================] - 1s 793us/step - loss: 0.2197 - accuracy: 0.9382\n",
      "Epoch 136/200\n",
      "1875/1875 [==============================] - 1s 788us/step - loss: 0.2194 - accuracy: 0.9385\n",
      "Epoch 137/200\n",
      "1875/1875 [==============================] - 1s 784us/step - loss: 0.2192 - accuracy: 0.9389\n",
      "Epoch 138/200\n",
      "1875/1875 [==============================] - 1s 771us/step - loss: 0.2194 - accuracy: 0.9385\n",
      "Epoch 139/200\n",
      "1875/1875 [==============================] - 1s 776us/step - loss: 0.2195 - accuracy: 0.9386\n",
      "Epoch 140/200\n",
      "1875/1875 [==============================] - 1s 746us/step - loss: 0.2194 - accuracy: 0.9386\n",
      "Epoch 141/200\n",
      "1875/1875 [==============================] - 1s 764us/step - loss: 0.2191 - accuracy: 0.9385\n",
      "Epoch 142/200\n",
      "1875/1875 [==============================] - 1s 777us/step - loss: 0.2189 - accuracy: 0.9391\n",
      "Epoch 143/200\n",
      "1875/1875 [==============================] - 2s 937us/step - loss: 0.2194 - accuracy: 0.9395\n",
      "Epoch 144/200\n",
      "1875/1875 [==============================] - 2s 864us/step - loss: 0.2194 - accuracy: 0.9385\n",
      "Epoch 145/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2188 - accuracy: 0.9395\n",
      "Epoch 146/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2190 - accuracy: 0.9386\n",
      "Epoch 147/200\n",
      "1875/1875 [==============================] - 2s 878us/step - loss: 0.2191 - accuracy: 0.9386\n",
      "Epoch 148/200\n",
      "1875/1875 [==============================] - 2s 915us/step - loss: 0.2191 - accuracy: 0.9391\n",
      "Epoch 149/200\n",
      "1875/1875 [==============================] - 2s 941us/step - loss: 0.2186 - accuracy: 0.9391\n",
      "Epoch 150/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2189 - accuracy: 0.9386\n",
      "Epoch 151/200\n",
      "1875/1875 [==============================] - 2s 808us/step - loss: 0.2188 - accuracy: 0.9391\n",
      "Epoch 152/200\n",
      "1875/1875 [==============================] - 1s 747us/step - loss: 0.2183 - accuracy: 0.9391\n",
      "Epoch 153/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2188 - accuracy: 0.9387\n",
      "Epoch 154/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2184 - accuracy: 0.9385\n",
      "Epoch 155/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2183 - accuracy: 0.9388\n",
      "Epoch 156/200\n",
      "1875/1875 [==============================] - 1s 747us/step - loss: 0.2187 - accuracy: 0.9384\n",
      "Epoch 157/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2184 - accuracy: 0.9392\n",
      "Epoch 158/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2182 - accuracy: 0.9390\n",
      "Epoch 159/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2180 - accuracy: 0.9391\n",
      "Epoch 160/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2184 - accuracy: 0.9387\n",
      "Epoch 161/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2184 - accuracy: 0.9384\n",
      "Epoch 162/200\n",
      "1875/1875 [==============================] - 1s 738us/step - loss: 0.2184 - accuracy: 0.9388\n",
      "Epoch 163/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2182 - accuracy: 0.9391\n",
      "Epoch 164/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2180 - accuracy: 0.9392\n",
      "Epoch 165/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2178 - accuracy: 0.9394\n",
      "Epoch 166/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2180 - accuracy: 0.9393\n",
      "Epoch 167/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2181 - accuracy: 0.9390\n",
      "Epoch 168/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2179 - accuracy: 0.9391\n",
      "Epoch 169/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2177 - accuracy: 0.9391\n",
      "Epoch 170/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2179 - accuracy: 0.9391\n",
      "Epoch 171/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2181 - accuracy: 0.9394\n",
      "Epoch 172/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2174 - accuracy: 0.9389\n",
      "Epoch 173/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2179 - accuracy: 0.9389\n",
      "Epoch 174/200\n",
      "1875/1875 [==============================] - 1s 729us/step - loss: 0.2173 - accuracy: 0.9387\n",
      "Epoch 175/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2177 - accuracy: 0.9385\n",
      "Epoch 176/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2177 - accuracy: 0.9394\n",
      "Epoch 177/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2174 - accuracy: 0.9391\n",
      "Epoch 178/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2177 - accuracy: 0.9395\n",
      "Epoch 179/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2175 - accuracy: 0.9394\n",
      "Epoch 180/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2177 - accuracy: 0.9386\n",
      "Epoch 181/200\n",
      "1875/1875 [==============================] - 1s 717us/step - loss: 0.2176 - accuracy: 0.9390\n",
      "Epoch 182/200\n",
      "1875/1875 [==============================] - 1s 741us/step - loss: 0.2170 - accuracy: 0.9398\n",
      "Epoch 183/200\n",
      "1875/1875 [==============================] - 1s 737us/step - loss: 0.2173 - accuracy: 0.9393\n",
      "Epoch 184/200\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.2174 - accuracy: 0.9394\n",
      "Epoch 185/200\n",
      "1875/1875 [==============================] - 1s 713us/step - loss: 0.2176 - accuracy: 0.9398\n",
      "Epoch 186/200\n",
      "1875/1875 [==============================] - 1s 718us/step - loss: 0.2172 - accuracy: 0.9389\n",
      "Epoch 187/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2175 - accuracy: 0.9390\n",
      "Epoch 188/200\n",
      "1875/1875 [==============================] - 1s 729us/step - loss: 0.2173 - accuracy: 0.9392\n",
      "Epoch 189/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2176 - accuracy: 0.9387\n",
      "Epoch 190/200\n",
      "1875/1875 [==============================] - 1s 730us/step - loss: 0.2170 - accuracy: 0.9402\n",
      "Epoch 191/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2173 - accuracy: 0.9393\n",
      "Epoch 192/200\n",
      "1875/1875 [==============================] - 1s 703us/step - loss: 0.2170 - accuracy: 0.9397\n",
      "Epoch 193/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2171 - accuracy: 0.9391\n",
      "Epoch 194/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2174 - accuracy: 0.9394\n",
      "Epoch 195/200\n",
      "1875/1875 [==============================] - 1s 721us/step - loss: 0.2170 - accuracy: 0.9387\n",
      "Epoch 196/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2170 - accuracy: 0.9392\n",
      "Epoch 197/200\n",
      "1875/1875 [==============================] - 1s 712us/step - loss: 0.2165 - accuracy: 0.9394\n",
      "Epoch 198/200\n",
      "1875/1875 [==============================] - 1s 703us/step - loss: 0.2169 - accuracy: 0.9393\n",
      "Epoch 199/200\n",
      "1875/1875 [==============================] - 1s 703us/step - loss: 0.2172 - accuracy: 0.9390\n",
      "Epoch 200/200\n",
      "1875/1875 [==============================] - 1s 738us/step - loss: 0.2166 - accuracy: 0.9393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a13aad1f50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape = (784,), activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train_flattened, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet defines, compiles, and trains a simple neural network using Keras to classify the flattened MNIST images into their respective digit classes. Let's break it down step by step:\n",
    "\n",
    "1. Model Definition:\n",
    "\n",
    "* `model = keras.Sequential([...])`: This creates a Sequential model in Keras, a linear stack of layers. In this case, it's a single-layer neural network.\n",
    "* `keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')`: This line adds a Dense layer to the model. It has 10 neurons (units) representing the output classes (digits 0 to 9). The `input_shape=(784,)` specifies that the input to this layer is a flattened array of length 784 (representing the pixels of the images). The activation function used in this layer is the sigmoid function `(activation='sigmoid')`.\n",
    "2. Model Compilation:\n",
    "\n",
    "* `model.compile(...)`: This compiles the model, specifying the optimizer, loss function, and metrics for evaluation.\n",
    "* `optimizer='adam'`: Adam is an optimization algorithm used to update network weights iteratively based on training data.\n",
    "* `loss='sparse_categorical_crossentropy'`: This is the loss function used to measure how well the model performs on the training data. It is suitable for integer-encoded labels (like in the MNIST dataset) and calculates the loss between the predicted and true labels.\n",
    "* `metrics=['accuracy']`: During training, the model will track accuracy as a metric to evaluate its performance.\n",
    "3. Model Training:\n",
    "\n",
    "* `model.fit(x_train_flattened, y_train, epochs=200)`: This trains the model using the training data `(x_train_flattened)` and their respective labels `(y_train)` for 200 epochs. The `fit()` function fits the model to the training data, adjusting the model's weights to minimize the specified loss function.\n",
    "  \n",
    "Overall, this code creates a simple neural network with one Dense layer, compiles it with appropriate settings for training on the MNIST dataset, and then trains it for 200 epochs using the flattened input images and their corresponding labels. The model will aim to learn how to predict the correct digit labels for the provided images during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 697us/step - loss: 0.3180 - accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31797608733177185, 0.9253000020980835]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet `model.evaluate(x_test_flattened, y_test)` is used to evaluate the trained model's performance on the testing dataset `(x_test_flattened)` to assess its accuracy and loss. This method computes the specified metrics defined during the model compilation.\n",
    "\n",
    "* `x_test_flattened`: The flattened test images.\n",
    "* `y_test`: The corresponding true labels for the test images.\n",
    "When this line of code is executed after the model has been trained, it will use the trained model to predict the labels for the test images and compare these predictions against the true labels `(y_test)`. It will calculate the specified metrics, in this case, accuracy, as defined during the model compilation.\n",
    "\n",
    "For instance, if you have defined accuracy as a metric during model compilation `(metrics=['accuracy'])`, this `model.evaluate()` function will output the accuracy of the model on the test dataset. It will display the accuracy achieved by the model in classifying the test images into their respective digit categories based on the predictions made by the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 596us/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_test_flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line `y_predicted = model.predict(x_test_flattened)` is used to generate predictions using the trained model on the test dataset `(x_test_flattened)`. This line predicts the output labels based on the input test data.\n",
    "\n",
    "Once this line is executed, `y_predicted` will contain the model's predictions for each test input. Each prediction will likely be an array of probabilities or scores indicating the likelihood or confidence of the input belonging to each class.\n",
    "\n",
    "If you then access `y_predicted[0]`, it will retrieve the model's prediction for the first test input in the form of an array of probabilities or scores. For the MNIST dataset, since the model was configured with a `'softmax'` activation function in the output layer (commonly used for multi-class classification problems), the values in `y_predicted[0]` represent the probabilities or confidence scores for each class (digits 0 to 9) that the model predicts for the first test input.\n",
    "\n",
    "For example, if you have ten values like `[0.1, 0.05, 0.03, 0.8, 0.01, 0.02, 0.05, 0.02, 0.03, 0.02]`, it means the model is most confident (highest probability value) that the image corresponds to the digit '3' because the third element has the highest probability (0.8) among all classes.\n",
    "\n",
    "So, `y_predicted[0]` will provide the model's predicted probabilities or confidence scores for the first test input in the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1382357e-14, 3.4129714e-18, 4.6121921e-12, 6.1559504e-01,\n",
       "       4.8002694e-06, 2.9649453e-03, 2.4804781e-22, 9.9871385e-01,\n",
       "       1.8284963e-03, 3.5573307e-02], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a157540850>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can see that the expected digit is 7. To confirm that, we can examine the output of this code, `y_predicted[0]`.\n",
    "\n",
    "It is evident that position at which the number is highest is the 7th position, hence, 7 as the expected output.\n",
    "\n",
    "That can be easily implemented using `np.argmax()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.argmax(y_predicted[0])` uses NumPy's argmax function to find the index of the maximum value in the array `y_predicted[0]`. This is commonly used when dealing with prediction probabilities or scores generated by a model for classification tasks.\n",
    "\n",
    "In the context of the MNIST dataset:\n",
    "\n",
    "`y_predicted[0]` is an array containing the model's predicted probabilities or scores for each class (digits 0 to 9) for the first test input.\n",
    "`np.argmax()` finds the index of the maximum value in that array, effectively identifying the class with the highest predicted probability or score.\n",
    "So, `np.argmax(y_predicted[0])` will return the index corresponding to the highest probability in the `y_predicted[0]` array, indicating the model's predicted label for the first test input in the MNIST dataset. This index represents the digit that the model predicts the image to be, based on its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The line of code `y_predicted_labels = [np.argmax(i) for i in y_predicted]` is using a list comprehension to iterate through each prediction array (i) in `y_predicted` and apply `np.argmax()` to find the index of the maximum value in each prediction array. This operation effectively determines the predicted label for each test input in the MNIST dataset.\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "`y_predicted` is a list/array containing predictions made by the model for all the test inputs. Each element (i) in `y_predicted` is an array of probabilities or scores for the classes (digits 0 to 9).\n",
    "`[np.argmax(i) for i in y_predicted]` iterates through each prediction array (i) in `y_predicted` and applies `np.argmax()` to find the index of the maximum value in each array.\n",
    "The result is stored in the list `y_predicted_labels`, which contains the predicted labels (digits) for each test input based on the model's predictions.\n",
    "Ultimately, after executing this line of code, `y_predicted_labels` will hold the predicted labels for each test input, inferred from the model's predictions. These predicted labels represent the digits the model believes each test image corresponds to, based on the highest probability class from its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=int32, numpy=\n",
       "array([[ 950,    0,    3,    3,    1,   11,    6,    3,    3,    0],\n",
       "       [   0, 1116,    6,    2,    0,    2,    3,    1,    5,    0],\n",
       "       [   5,   13,  916,   21,   10,    3,   11,    8,   42,    3],\n",
       "       [   3,    2,   12,  927,    3,   19,    2,   10,   27,    5],\n",
       "       [   1,    4,    7,    4,  927,    0,    6,    6,    6,   21],\n",
       "       [   8,    5,    3,   37,   10,  772,   14,    6,   33,    4],\n",
       "       [   9,    3,   10,    2,    5,   14,  913,    1,    1,    0],\n",
       "       [   0,   10,   22,    9,    4,    2,    0,  950,    3,   28],\n",
       "       [   6,   12,    6,   21,    7,   23,   10,   12,  870,    7],\n",
       "       [   7,    7,    1,    8,   25,    8,    0,   30,   11,  912]])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = tf.math.confusion_matrix(labels = y_test, predictions = y_predicted_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code `tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)` generates a confusion matrix using TensorFlow's `tf.math.confusion_matrix` function. This matrix provides a summary of the model's performance in a classification task by comparing the true labels (y_test) against the predicted labels `(y_predicted_labels)` for the test dataset.\n",
    "\n",
    "* `labels=y_test`: This parameter expects the true labels of the test dataset (y_test), which are the actual digits corresponding to the test images.\n",
    "* `predictions=y_predicted_labels`: This parameter takes the predicted labels (y_predicted_labels) generated by the model for the test dataset.\n",
    "The confusion matrix is a table that shows the counts of true positive, true negative, false positive, and false negative predictions for each class (in this case, digits 0 to 9). It helps to evaluate the performance of a classification model by revealing where the model is making correct or incorrect predictions.\n",
    "\n",
    "Running `cm = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)` will compute the confusion matrix based on the provided true labels `(y_test)` and predicted labels `(y_predicted_labels)`. The resulting `cm` variable will hold the confusion matrix as a TensorFlow tensor, showing the classification performance of the model across different digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize = (10,15))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seaborn** is a statistical data visualization librarybuilt on top of Matplotlib. It offers a higher-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "The `plt.figure()` command from Matplotlib creates a new figure, or a space where a plot can be drawn.\n",
    "\n",
    "`figsize=(10,15)` is a parameter that sets the size of the figure in inches. In this case, the width of the figure is 10 inches, and the height is 15 inches.\n",
    "\n",
    "`sns.heatmap()` is a function in Seaborn that creates a heatmap. A heatmap is a two-dimensional graphical representation of data where individual values contained in a matrix are represented as colors.\n",
    "\n",
    "`cm` is the confusion matrix to be viusalized. This has already been defined in the previous code block. A confusion matrix is a table used to describe the performance of a classification model. Each entry in the matrix corresponds to the count of predictions for each class.\n",
    "\n",
    "`annot=True` tells Seaborn to annotate each cell in the heatmap with the numeric value using text.\n",
    "\n",
    "Let's work with some hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2328 - accuracy: 0.9321\n",
      "Epoch 2/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0990 - accuracy: 0.9700\n",
      "Epoch 3/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0715 - accuracy: 0.9778\n",
      "Epoch 4/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0544 - accuracy: 0.9822\n",
      "Epoch 5/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0434 - accuracy: 0.9858\n",
      "Epoch 6/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0356 - accuracy: 0.9886\n",
      "Epoch 7/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0274 - accuracy: 0.9905\n",
      "Epoch 8/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0279 - accuracy: 0.9906\n",
      "Epoch 9/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0222 - accuracy: 0.9924\n",
      "Epoch 10/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0196 - accuracy: 0.9936\n",
      "Epoch 11/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0181 - accuracy: 0.9937\n",
      "Epoch 12/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0161 - accuracy: 0.9946\n",
      "Epoch 13/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0145 - accuracy: 0.9950\n",
      "Epoch 14/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 15/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0138 - accuracy: 0.9957\n",
      "Epoch 16/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0123 - accuracy: 0.9960\n",
      "Epoch 17/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0114 - accuracy: 0.9965\n",
      "Epoch 18/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 19/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0103 - accuracy: 0.9965\n",
      "Epoch 20/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0124 - accuracy: 0.9958\n",
      "Epoch 21/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0103 - accuracy: 0.9969\n",
      "Epoch 22/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0099 - accuracy: 0.9970\n",
      "Epoch 23/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0080 - accuracy: 0.9977\n",
      "Epoch 24/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0106 - accuracy: 0.9967\n",
      "Epoch 25/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0085 - accuracy: 0.9976\n",
      "Epoch 26/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9973\n",
      "Epoch 27/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0092 - accuracy: 0.9974\n",
      "Epoch 28/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0087 - accuracy: 0.9976\n",
      "Epoch 29/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9975\n",
      "Epoch 30/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0084 - accuracy: 0.9976\n",
      "Epoch 31/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 32/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9977\n",
      "Epoch 33/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9977\n",
      "Epoch 34/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0081 - accuracy: 0.9979\n",
      "Epoch 35/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0069 - accuracy: 0.9982\n",
      "Epoch 36/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0079 - accuracy: 0.9977\n",
      "Epoch 37/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 38/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 39/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 40/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 41/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9977\n",
      "Epoch 42/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0085 - accuracy: 0.9977\n",
      "Epoch 43/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0048 - accuracy: 0.9984\n",
      "Epoch 44/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0075 - accuracy: 0.9979\n",
      "Epoch 45/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0078 - accuracy: 0.9980\n",
      "Epoch 46/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 47/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9987\n",
      "Epoch 48/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9977\n",
      "Epoch 49/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0037 - accuracy: 0.9989\n",
      "Epoch 50/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0076 - accuracy: 0.9980\n",
      "Epoch 51/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0064 - accuracy: 0.9983\n",
      "Epoch 52/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9985\n",
      "Epoch 53/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 54/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0080 - accuracy: 0.9979\n",
      "Epoch 55/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0063 - accuracy: 0.9985\n",
      "Epoch 56/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0057 - accuracy: 0.9986\n",
      "Epoch 57/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9985\n",
      "Epoch 58/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 59/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0081 - accuracy: 0.9981\n",
      "Epoch 60/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 61/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9985\n",
      "Epoch 62/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9987\n",
      "Epoch 63/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0071 - accuracy: 0.9980\n",
      "Epoch 64/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0074 - accuracy: 0.9985\n",
      "Epoch 65/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9986\n",
      "Epoch 66/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0058 - accuracy: 0.9986\n",
      "Epoch 67/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 68/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 69/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9987\n",
      "Epoch 70/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9989\n",
      "Epoch 71/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 72/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0076 - accuracy: 0.9985\n",
      "Epoch 73/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 74/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 75/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9984\n",
      "Epoch 76/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9987\n",
      "Epoch 77/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0081 - accuracy: 0.9984\n",
      "Epoch 78/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0052 - accuracy: 0.9988\n",
      "Epoch 79/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9988\n",
      "Epoch 80/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 81/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0042 - accuracy: 0.9992\n",
      "Epoch 82/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9986\n",
      "Epoch 83/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0039 - accuracy: 0.9991\n",
      "Epoch 84/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 85/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0060 - accuracy: 0.9988\n",
      "Epoch 86/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9990\n",
      "Epoch 87/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0063 - accuracy: 0.9986\n",
      "Epoch 88/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 89/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9987\n",
      "Epoch 90/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0055 - accuracy: 0.9987\n",
      "Epoch 91/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053 - accuracy: 0.9987\n",
      "Epoch 92/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 93/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9985\n",
      "Epoch 94/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 95/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 96/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9991\n",
      "Epoch 97/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 98/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9990\n",
      "Epoch 99/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 100/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0077 - accuracy: 0.9985\n",
      "Epoch 101/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 102/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 0.9992\n",
      "Epoch 103/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 104/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0052 - accuracy: 0.9990\n",
      "Epoch 105/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0070 - accuracy: 0.9985\n",
      "Epoch 106/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 107/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9986\n",
      "Epoch 108/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 109/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0071 - accuracy: 0.9987\n",
      "Epoch 110/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 111/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 112/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0061 - accuracy: 0.9987\n",
      "Epoch 113/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9992\n",
      "Epoch 114/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 0.9992\n",
      "Epoch 115/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0079 - accuracy: 0.9987\n",
      "Epoch 116/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 117/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9991\n",
      "Epoch 118/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 119/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9989\n",
      "Epoch 120/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0065 - accuracy: 0.9989\n",
      "Epoch 121/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 122/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 123/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 124/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0089 - accuracy: 0.9987\n",
      "Epoch 125/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0067 - accuracy: 0.9991\n",
      "Epoch 126/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 127/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0057 - accuracy: 0.9991\n",
      "Epoch 128/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 129/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0072 - accuracy: 0.9988\n",
      "Epoch 130/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0040 - accuracy: 0.9993\n",
      "Epoch 131/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0071 - accuracy: 0.9987\n",
      "Epoch 132/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 133/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 134/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0042 - accuracy: 0.9992\n",
      "Epoch 135/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 136/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0049 - accuracy: 0.9993\n",
      "Epoch 137/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9991\n",
      "Epoch 138/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 139/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 140/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0058 - accuracy: 0.9991\n",
      "Epoch 141/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0039 - accuracy: 0.9993\n",
      "Epoch 142/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0063 - accuracy: 0.9990\n",
      "Epoch 143/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0066 - accuracy: 0.9991\n",
      "Epoch 144/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 145/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 146/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 147/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0058 - accuracy: 0.9992\n",
      "Epoch 148/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0064 - accuracy: 0.9991\n",
      "Epoch 149/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 150/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0059 - accuracy: 0.9991\n",
      "Epoch 151/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0048 - accuracy: 0.9993\n",
      "Epoch 152/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 153/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0070 - accuracy: 0.9991\n",
      "Epoch 154/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0061 - accuracy: 0.9992\n",
      "Epoch 155/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 156/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0048 - accuracy: 0.9993\n",
      "Epoch 157/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 158/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 159/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9992\n",
      "Epoch 160/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 161/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0051 - accuracy: 0.9993\n",
      "Epoch 162/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0073 - accuracy: 0.9992\n",
      "Epoch 163/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 164/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0055 - accuracy: 0.9993\n",
      "Epoch 165/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0062 - accuracy: 0.9991\n",
      "Epoch 166/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9991\n",
      "Epoch 167/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9997\n",
      "Epoch 168/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0068 - accuracy: 0.9991\n",
      "Epoch 169/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0071 - accuracy: 0.9990\n",
      "Epoch 170/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 171/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0058 - accuracy: 0.9992\n",
      "Epoch 172/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 173/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 174/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0040 - accuracy: 0.9995\n",
      "Epoch 175/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 176/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0047 - accuracy: 0.9993\n",
      "Epoch 177/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 178/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9992\n",
      "Epoch 179/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0065 - accuracy: 0.9991\n",
      "Epoch 180/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 9.9204e-04 - accuracy: 0.9998\n",
      "Epoch 181/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0083 - accuracy: 0.9991\n",
      "Epoch 182/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0055 - accuracy: 0.9993\n",
      "Epoch 183/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0048 - accuracy: 0.9994\n",
      "Epoch 184/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 185/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 186/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0056 - accuracy: 0.9993\n",
      "Epoch 187/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 188/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0053 - accuracy: 0.9993\n",
      "Epoch 189/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0030 - accuracy: 0.9996\n",
      "Epoch 190/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0061 - accuracy: 0.9993\n",
      "Epoch 191/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 192/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 193/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 194/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0060 - accuracy: 0.9991\n",
      "Epoch 195/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 196/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 197/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0082 - accuracy: 0.9991\n",
      "Epoch 198/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 199/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0030 - accuracy: 0.9996\n",
      "Epoch 200/200\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0069 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a17ab42b90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape = (784,), activation = 'relu'),\n",
    "    keras.layers.Dense(150, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train_flattened, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This revised code defines a more complex neural network compared to the previous one. Let's break down the changes:\n",
    "\n",
    "1. Model Architecture:\n",
    "\n",
    "* Three Dense Layers: This model has three dense layers.\n",
    "* `keras.layers.Dense(100, input_shape = (784,), activation = 'relu')`: The first dense layer consists of 100 neurons and uses the ReLU (Rectified Linear Unit) activation function. It introduces non-linearity to the model, allowing it to learn more complex patterns. ReLU is preferred because it helps in mitigating the vanishing gradient problem and speeds up training. The `input_shape = (784,)` specifies that each input sample is a 784-dimensional vector, which corresponds to the flattened 28x28 images from the MNIST dataset.\n",
    "* `keras.layers.Dense(150, activation = 'relu')`: This second dense layer introduces even more neurons (150), thereby increasing the model's capacity to learn. It also uses the ReLU activation function to introduce more non-linearity.\n",
    "* `keras.layers.Dense(10, activation='sigmoid')`: The third dense layer has 10 neurons (representing the output classes) and uses the sigmoid activation function. This layer generates the output probabilities for each class using the softmax activation. I decided to change the activation function from sigmoid to softmax. For multi-class classification like MNIST, `softmax` activation is typically used in the output layer because it converts the outputs to probability distribution across the classes.\n",
    "2. Model Compilation and Training:\n",
    "\n",
    "The compilation and training settings `(optimizer, loss, metrics, and epochs)` remain the same as before.\n",
    "This updated model introduces more complexity by adding an additional hidden layer with 100 neurons and ReLU activation. The ReLU activation function is often used in hidden layers to introduce non-linearity and capture more intricate patterns in the data.\n",
    "\n",
    "The model will undergo training for 200 epochs using the training dataset `(x_train_flattened and y_train)`, aiming to learn the representations of handwritten digits from the MNIST dataset. This complex architecture might potentially improve the model's ability to learn intricate patterns and enhance its classification performance compared to simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 825us/step - loss: 0.5448 - accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5448419451713562, 0.9818000197410583]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_flattened, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
