{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Convolutional Neural Network Using the Fashion MNIST Dataset\n",
    "\n",
    "Welcome to this introductory tutorial on Convolutional Neural Networks (CNNs) using the Fashion MNIST dataset. This tutorial is designed for beginners who are new to neural networks and TensorFlow, a powerful library for machine learning. By the end of this guide, you will have a basic understanding of how to build and train a CNN to classify clothing images.\n",
    "\n",
    "### What is the Fashion MNIST Dataset?\n",
    "\n",
    "Fashion MNIST is a dataset developed as a more challenging replacement for the traditional MNIST dataset. Instead of handwritten digits, it contains 70,000 grayscale images of 10 fashion categories. Each image is 28x28 pixels and represents a specific type of clothing, such as T-shirts, dresses, and shoes. This dataset is widely used for benchmarking machine learning algorithms.\n",
    "\n",
    "In this tutorial, we will first implement the classical artificial neural network before implementing the convolutional neural network.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Basic understanding of Python programming\n",
    "* TensorFlow and Keras installed. (TensorFlow 2.x is recommended as it includes Keras by default.)\n",
    "\n",
    "### Setting Up Your Environment\n",
    "\n",
    "First, we'll import the necessary libraries:\n",
    "\n",
    "* `TensorFlow` for building and training the neural network.\n",
    "* `Keras`, a high-level neural networks API, which is now integrated into TensorFlow.\n",
    "* `NumPy` for numerical operations.\n",
    "* `Matplotlib` for plotting and visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `fashion_mnist = keras.datasets.fashion_mnist`: This line is doing two things:\n",
    "    - `keras.datasets.fashion_mnist` refers to the Fashion MNIST dataset which is part of the datasets available in Keras. The Fashion MNIST dataset is a collection of 70,000 grayscale images of 10 different types of clothing items, like shirts, dresses, and shoes.\n",
    "    - `fashion_mnist =` is assigning this dataset to a variable named `fashion_mnist`. This means that we can use the variable `fashion_mnist` to access the dataset.\n",
    "\n",
    "2. `(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()`: This line is where the dataset is actually loaded into the program. Let's break it down:\n",
    "    - `fashion_mnist.load_data()` is a function call that loads the Fashion MNIST dataset. It returns four arrays:\n",
    "        - `x_train`: The images that you'll use to train the neural network. Each image is represented as a 28x28 array of grayscale pixel values.\n",
    "        - `y_train`: The labels for the training images. Each label is an integer from 0 to 9, corresponding to the type of clothing item in the image (e.g., 0 might be a T-shirt/top, 1 might be a trouser, etc.).\n",
    "        - `x_test`: The images that you'll use to test the neural network. Like `x_train`, these are 28x28 arrays of grayscale pixel values.\n",
    "        - `y_test`: The labels for the test images, using the same integer coding as `y_train`.\n",
    "    - `(x_train, y_train), (x_test, y_test) =` is a form of multiple assignment in Python. It assigns the returned arrays to the variables `x_train`, `y_train`, `x_test`, and `y_test` respectively. This way, you have your training data (`x_train`, `y_train`) and your testing data (`x_test`, `y_test`) neatly separated and labeled for use in training and evaluating a machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `x_train = x_train / 255.0`: \n",
    "    - This line of code performs normalization on the training dataset (`x_train`). \n",
    "    - Each image in `x_train` is represented as a 28x28 array with pixel values ranging from 0 to 255 (since the images are in grayscale). \n",
    "    - Dividing each pixel value by 255 scales these values to a range between 0 and 1. \n",
    "    - This scaling is important because it helps in reducing the complexity of the model's computations and makes the training process faster and more stable.\n",
    "\n",
    "2. `x_test = x_test / 255.0`: \n",
    "    - Similarly, this line normalizes the testing dataset (`x_test`). \n",
    "    - Just like the training dataset, the pixel values in the test images are also divided by 255 to bring them into the range of 0 to 1. \n",
    "    - It's crucial to apply the same normalization to both training and testing data to ensure consistency in the data that the model sees during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "x_train_flattened = x_train.reshape(len(x_train), 28*28)\n",
    "x_test_flattened = x_test.reshape(len(x_test), 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `x_train_flattened = x_train.reshape(len(x_train), 28*28)`:\n",
    "    - `x_train.reshape(len(x_train), 28*28)` is a function call that reshapes the training dataset (`x_train`).\n",
    "    - Each image in `x_train` is currently a 28x28 array of pixel values. This line reshapes each image into a 1D array of 784 elements (since 28*28 = 784).\n",
    "    - `len(x_train)` is used to maintain the number of images in the dataset. So, the shape of `x_train_flattened` will be [number of images x 784].\n",
    "    - This transformation is necessary for certain types of neural networks that require input to be a 1D array, such as a basic dense (fully connected) network.\n",
    "\n",
    "2. `x_test_flattened = x_test.reshape(len(x_test), 28*28)`:\n",
    "    - Similarly, this line reshapes the testing dataset (`x_test`).\n",
    "    - Each image in `x_test` is also converted from a 28x28 array into a 1D array of 784 elements.\n",
    "    - Itâ€™s important to apply the same transformation to both the training and testing datasets to ensure consistency in the model's input format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4858 - accuracy: 0.8252\n",
      "Epoch 2/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3638 - accuracy: 0.8664\n",
      "Epoch 3/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3293 - accuracy: 0.8787\n",
      "Epoch 4/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3061 - accuracy: 0.8860\n",
      "Epoch 5/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2901 - accuracy: 0.8923\n",
      "Epoch 6/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2738 - accuracy: 0.8967\n",
      "Epoch 7/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2617 - accuracy: 0.9018\n",
      "Epoch 8/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2520 - accuracy: 0.9055\n",
      "Epoch 9/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2438 - accuracy: 0.9086\n",
      "Epoch 10/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2346 - accuracy: 0.9112\n",
      "Epoch 11/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2256 - accuracy: 0.9135\n",
      "Epoch 12/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2194 - accuracy: 0.9166\n",
      "Epoch 13/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2122 - accuracy: 0.9199\n",
      "Epoch 14/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2075 - accuracy: 0.9206\n",
      "Epoch 15/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1984 - accuracy: 0.9250\n",
      "Epoch 16/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1925 - accuracy: 0.9267\n",
      "Epoch 17/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1859 - accuracy: 0.9294\n",
      "Epoch 18/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1838 - accuracy: 0.9288\n",
      "Epoch 19/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1768 - accuracy: 0.9326\n",
      "Epoch 20/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1728 - accuracy: 0.9323\n",
      "Epoch 21/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1717 - accuracy: 0.9347\n",
      "Epoch 22/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1631 - accuracy: 0.9370\n",
      "Epoch 23/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1605 - accuracy: 0.9378\n",
      "Epoch 24/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1553 - accuracy: 0.9409\n",
      "Epoch 25/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1522 - accuracy: 0.9414\n",
      "Epoch 26/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1482 - accuracy: 0.9436\n",
      "Epoch 27/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1469 - accuracy: 0.9439\n",
      "Epoch 28/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1408 - accuracy: 0.9459\n",
      "Epoch 29/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1395 - accuracy: 0.9467\n",
      "Epoch 30/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1385 - accuracy: 0.9469\n",
      "Epoch 31/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1317 - accuracy: 0.9494\n",
      "Epoch 32/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1284 - accuracy: 0.9507\n",
      "Epoch 33/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1280 - accuracy: 0.9506\n",
      "Epoch 34/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1237 - accuracy: 0.9530\n",
      "Epoch 35/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1244 - accuracy: 0.9528\n",
      "Epoch 36/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1194 - accuracy: 0.9534\n",
      "Epoch 37/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1185 - accuracy: 0.9546\n",
      "Epoch 38/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1138 - accuracy: 0.9559\n",
      "Epoch 39/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1134 - accuracy: 0.9568\n",
      "Epoch 40/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1107 - accuracy: 0.9573\n",
      "Epoch 41/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1132 - accuracy: 0.9564\n",
      "Epoch 42/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1055 - accuracy: 0.9592\n",
      "Epoch 43/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1071 - accuracy: 0.9591\n",
      "Epoch 44/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1042 - accuracy: 0.9606\n",
      "Epoch 45/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1007 - accuracy: 0.9617\n",
      "Epoch 46/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0989 - accuracy: 0.9619\n",
      "Epoch 47/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0964 - accuracy: 0.9641\n",
      "Epoch 48/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1001 - accuracy: 0.9615\n",
      "Epoch 49/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0951 - accuracy: 0.9634\n",
      "Epoch 50/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0953 - accuracy: 0.9632\n",
      "Epoch 51/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0926 - accuracy: 0.9648\n",
      "Epoch 52/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0898 - accuracy: 0.9658\n",
      "Epoch 53/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0893 - accuracy: 0.9668\n",
      "Epoch 54/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0914 - accuracy: 0.9659\n",
      "Epoch 55/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0859 - accuracy: 0.9672\n",
      "Epoch 56/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0863 - accuracy: 0.9671\n",
      "Epoch 57/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0832 - accuracy: 0.9684\n",
      "Epoch 58/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0848 - accuracy: 0.9680\n",
      "Epoch 59/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0832 - accuracy: 0.9684\n",
      "Epoch 60/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0802 - accuracy: 0.9693\n",
      "Epoch 61/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0783 - accuracy: 0.9701\n",
      "Epoch 62/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0824 - accuracy: 0.9687\n",
      "Epoch 63/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0776 - accuracy: 0.9696\n",
      "Epoch 64/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0782 - accuracy: 0.9706\n",
      "Epoch 65/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0802 - accuracy: 0.9696\n",
      "Epoch 66/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0721 - accuracy: 0.9722\n",
      "Epoch 67/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0765 - accuracy: 0.9716\n",
      "Epoch 68/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0719 - accuracy: 0.9732\n",
      "Epoch 69/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0758 - accuracy: 0.9718\n",
      "Epoch 70/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0719 - accuracy: 0.9728\n",
      "Epoch 71/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0695 - accuracy: 0.9739\n",
      "Epoch 72/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0668 - accuracy: 0.9747\n",
      "Epoch 73/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0688 - accuracy: 0.9744\n",
      "Epoch 74/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0698 - accuracy: 0.9744\n",
      "Epoch 75/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0667 - accuracy: 0.9750\n",
      "Epoch 76/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0680 - accuracy: 0.9748\n",
      "Epoch 77/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0668 - accuracy: 0.9751\n",
      "Epoch 78/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0650 - accuracy: 0.9764\n",
      "Epoch 79/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0663 - accuracy: 0.9750\n",
      "Epoch 80/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0638 - accuracy: 0.9765\n",
      "Epoch 81/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0611 - accuracy: 0.9778\n",
      "Epoch 82/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0651 - accuracy: 0.9757\n",
      "Epoch 83/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0594 - accuracy: 0.9776\n",
      "Epoch 84/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0632 - accuracy: 0.9771\n",
      "Epoch 85/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0611 - accuracy: 0.9768\n",
      "Epoch 86/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0574 - accuracy: 0.9787\n",
      "Epoch 87/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0619 - accuracy: 0.9775\n",
      "Epoch 88/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0572 - accuracy: 0.9788\n",
      "Epoch 89/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0575 - accuracy: 0.9793\n",
      "Epoch 90/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0622 - accuracy: 0.9776\n",
      "Epoch 91/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0557 - accuracy: 0.9796\n",
      "Epoch 92/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0582 - accuracy: 0.9780\n",
      "Epoch 93/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0572 - accuracy: 0.9793\n",
      "Epoch 94/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0547 - accuracy: 0.9795\n",
      "Epoch 95/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0586 - accuracy: 0.9786\n",
      "Epoch 96/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0533 - accuracy: 0.9798\n",
      "Epoch 97/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0575 - accuracy: 0.9792\n",
      "Epoch 98/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0531 - accuracy: 0.9807\n",
      "Epoch 99/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0539 - accuracy: 0.9804\n",
      "Epoch 100/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0540 - accuracy: 0.9799\n",
      "Epoch 101/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0510 - accuracy: 0.9803\n",
      "Epoch 102/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0482 - accuracy: 0.9823\n",
      "Epoch 103/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0568 - accuracy: 0.9799\n",
      "Epoch 104/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0497 - accuracy: 0.9813\n",
      "Epoch 105/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0576 - accuracy: 0.9789\n",
      "Epoch 106/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0528 - accuracy: 0.9810\n",
      "Epoch 107/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0492 - accuracy: 0.9826\n",
      "Epoch 108/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0490 - accuracy: 0.9822\n",
      "Epoch 109/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0519 - accuracy: 0.9813\n",
      "Epoch 110/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0479 - accuracy: 0.9828\n",
      "Epoch 111/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0510 - accuracy: 0.9819\n",
      "Epoch 112/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0493 - accuracy: 0.9817\n",
      "Epoch 113/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0463 - accuracy: 0.9834\n",
      "Epoch 114/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0512 - accuracy: 0.9819\n",
      "Epoch 115/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0473 - accuracy: 0.9829\n",
      "Epoch 116/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0483 - accuracy: 0.9826\n",
      "Epoch 117/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0491 - accuracy: 0.9825\n",
      "Epoch 118/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0446 - accuracy: 0.9837\n",
      "Epoch 119/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0487 - accuracy: 0.9823\n",
      "Epoch 120/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0471 - accuracy: 0.9832\n",
      "Epoch 121/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0470 - accuracy: 0.9831\n",
      "Epoch 122/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0443 - accuracy: 0.9844\n",
      "Epoch 123/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0455 - accuracy: 0.9843\n",
      "Epoch 124/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0452 - accuracy: 0.9843\n",
      "Epoch 125/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0463 - accuracy: 0.9835\n",
      "Epoch 126/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0435 - accuracy: 0.9837\n",
      "Epoch 127/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0425 - accuracy: 0.9851\n",
      "Epoch 128/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0448 - accuracy: 0.9842\n",
      "Epoch 129/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0419 - accuracy: 0.9851\n",
      "Epoch 130/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0451 - accuracy: 0.9840\n",
      "Epoch 131/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0442 - accuracy: 0.9840\n",
      "Epoch 132/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0398 - accuracy: 0.9860\n",
      "Epoch 133/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0486 - accuracy: 0.9836\n",
      "Epoch 134/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0385 - accuracy: 0.9858\n",
      "Epoch 135/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0449 - accuracy: 0.9848\n",
      "Epoch 136/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0437 - accuracy: 0.9846\n",
      "Epoch 137/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0457 - accuracy: 0.9848\n",
      "Epoch 138/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0357 - accuracy: 0.9873\n",
      "Epoch 139/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0445 - accuracy: 0.9847\n",
      "Epoch 140/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0379 - accuracy: 0.9869\n",
      "Epoch 141/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0450 - accuracy: 0.9845\n",
      "Epoch 142/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0378 - accuracy: 0.9867\n",
      "Epoch 143/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0424 - accuracy: 0.9862\n",
      "Epoch 144/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0408 - accuracy: 0.9857\n",
      "Epoch 145/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0401 - accuracy: 0.9866\n",
      "Epoch 146/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0394 - accuracy: 0.9861\n",
      "Epoch 147/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0395 - accuracy: 0.9865\n",
      "Epoch 148/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0449 - accuracy: 0.9844\n",
      "Epoch 149/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0375 - accuracy: 0.9866\n",
      "Epoch 150/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0364 - accuracy: 0.9874\n",
      "Epoch 151/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0399 - accuracy: 0.9861\n",
      "Epoch 152/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0410 - accuracy: 0.9856\n",
      "Epoch 153/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0374 - accuracy: 0.9870\n",
      "Epoch 154/200\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0409 - accuracy: 0.9864\n",
      "Epoch 155/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0413 - accuracy: 0.9861\n",
      "Epoch 156/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0373 - accuracy: 0.9877\n",
      "Epoch 157/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0434 - accuracy: 0.9856\n",
      "Epoch 158/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0351 - accuracy: 0.9885\n",
      "Epoch 159/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0401 - accuracy: 0.9859\n",
      "Epoch 160/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0330 - accuracy: 0.9883\n",
      "Epoch 161/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0403 - accuracy: 0.9865\n",
      "Epoch 162/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0392 - accuracy: 0.9872\n",
      "Epoch 163/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0319 - accuracy: 0.9887\n",
      "Epoch 164/200\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0371 - accuracy: 0.9875\n",
      "Epoch 165/200\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0355 - accuracy: 0.9876\n",
      "Epoch 166/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0362 - accuracy: 0.9876\n",
      "Epoch 167/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0346 - accuracy: 0.9877\n",
      "Epoch 168/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0330 - accuracy: 0.9883\n",
      "Epoch 169/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0406 - accuracy: 0.9866\n",
      "Epoch 170/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0391 - accuracy: 0.9872\n",
      "Epoch 171/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0307 - accuracy: 0.9894\n",
      "Epoch 172/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9884\n",
      "Epoch 173/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0415 - accuracy: 0.9868\n",
      "Epoch 174/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0434 - accuracy: 0.9868\n",
      "Epoch 175/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0328 - accuracy: 0.9894\n",
      "Epoch 176/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0333 - accuracy: 0.9887\n",
      "Epoch 177/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0310 - accuracy: 0.9891\n",
      "Epoch 178/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0387 - accuracy: 0.9873\n",
      "Epoch 179/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0308 - accuracy: 0.9895\n",
      "Epoch 180/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9879\n",
      "Epoch 181/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0351 - accuracy: 0.9892\n",
      "Epoch 182/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0354 - accuracy: 0.9882\n",
      "Epoch 183/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0361 - accuracy: 0.9879\n",
      "Epoch 184/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0363 - accuracy: 0.9886\n",
      "Epoch 185/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0305 - accuracy: 0.9898\n",
      "Epoch 186/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0302 - accuracy: 0.9897\n",
      "Epoch 187/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0411 - accuracy: 0.9870\n",
      "Epoch 188/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0284 - accuracy: 0.9901\n",
      "Epoch 189/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0365 - accuracy: 0.9884\n",
      "Epoch 190/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0345 - accuracy: 0.9890\n",
      "Epoch 191/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0334 - accuracy: 0.9889\n",
      "Epoch 192/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0300 - accuracy: 0.9895\n",
      "Epoch 193/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0362 - accuracy: 0.9883\n",
      "Epoch 194/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0307 - accuracy: 0.9901\n",
      "Epoch 195/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9889\n",
      "Epoch 196/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0311 - accuracy: 0.9901\n",
      "Epoch 197/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0361 - accuracy: 0.9887\n",
      "Epoch 198/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0331 - accuracy: 0.9892\n",
      "Epoch 199/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0330 - accuracy: 0.9894\n",
      "Epoch 200/200\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0356 - accuracy: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25c2c8ebe10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Neural Network Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(100, input_shape=(784,), activation='relu'),\n",
    "    keras.layers.Dense(150, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')  # Change to softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "model.fit(x_train_flattened, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network Model\n",
    "\n",
    "1. `model = keras.Sequential([...])`:\n",
    "    - This line of code creates a neural network model.\n",
    "    - `keras.Sequential` is used to define a linear stack of layers in the neural network.\n",
    "    - Inside the `Sequential` constructor, we define the layers of the network:\n",
    "        - `keras.layers.Dense(100, input_shape=(784,), activation='relu')`: This is the first layer of the network, a dense (fully connected) layer with 100 neurons. `input_shape=(784,)` specifies the shape of the input data (flattened 784 elements from the 28x28 images). `activation='relu'` sets the Rectified Linear Unit (ReLU) activation function for this layer.\n",
    "        - `keras.layers.Dense(150, activation='relu')`: This is the second dense layer with 150 neurons and ReLU activation function.\n",
    "        - `keras.layers.Dense(10, activation='softmax')`: This is the output layer of the network. It has 10 neurons (one for each class in the dataset). The `softmax` activation function is used for multi-class classification, converting the output to probability scores for each class.\n",
    "\n",
    "### Compile the Model\n",
    "\n",
    "2. `model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])`:\n",
    "    - This line compiles the model, preparing it for training.\n",
    "    - `optimizer='adam'` sets Adam as the optimization algorithm. Adam is an efficient optimization algorithm that adjusts the weights of the network to minimize the loss function.\n",
    "    - `loss='sparse_categorical_crossentropy'` specifies the loss function. In multi-class classification tasks like this, sparse categorical crossentropy is commonly used.\n",
    "    - `metrics=['accuracy']` means that the model will track the accuracy metric during training and evaluation.\n",
    "\n",
    "### Train the Model\n",
    "\n",
    "3. `model.fit(x_train_flattened, y_train, epochs=200)`:\n",
    "    - This line trains the model using the training data.\n",
    "    - `model.fit` is the function to start the training process. `x_train_flattened` and `y_train` are the training data and labels, respectively.\n",
    "    - `epochs=200` specifies that the training process will run for 200 epochs. An epoch is one complete presentation of the data set to be learned to the learning machine. Training for more epochs can lead to a better model but also risks overfitting if too high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19da23b6e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 150)               15150     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1510      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95160 (371.72 KB)\n",
      "Trainable params: 95160 (371.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19da23fe510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolutional Neural Network\n",
    "\n",
    "Convolutional Neural Network is a specialized kind of neural network designed for processing data with a known grid-like topology, such as image data(2D grids of pixels) or time series data(1D grids).\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Convolution Operation:** At its core, CNNs utilize convolution, a specialized kind of linear operation. Convolution involves applying a kernel (or filter) to the input data to extract features. For instance, in image processing, this could mean highlighting edges or textures.\n",
    "\n",
    "2. **Pooling:** CNNs often employ pooling to reduce the spatial dimensions (height and width) of the feature maps, thereby reducing computation and model parameters.\n",
    "\n",
    "3. **Sparse Interactions and Parameter Sharing:** CNNs use smaller kernels to interact with portions of the input, leading to sparse interactions. This, along with parameter sharing (using the same kernel across the input), makes CNNs computationally efficient and reduces the number of parameters.\n",
    "\n",
    "4. **Equivariant Representations:** CNNs are designed to be equivariant to input transformations, meaning if the input is shifted or transformed, the output shifts in the same way. This property is particularly useful for tasks like image and speech recognition, where the relative position of features is more important than their absolute position.\n",
    "\n",
    "5. **Handling Variable Input Sizes:** CNNs can handle inputs of variable sizes, making them versatile for different types of data.\n",
    "\n",
    "6. **Applications:** CNNs have been tremendously successful in practical applications, particularly in areas like image and speech recognition. \n",
    "\n",
    "### Why \"Convolutional\":\n",
    "\n",
    "The \"convolutional\" part of the name comes from the mathematical operation called convolution. Don't get intimidated by this term! In the context of a CNN, convolution is essentially a way of filtering your data. Imagine sliding a small window across an image â€“ at each window position, the convolution operation involves multiplying the values of your window with the values in the image where the window is, summing them all up, and then putting this sum into a new image. This process is repeated across the whole image.\n",
    "\n",
    "### CNNs and Images:\n",
    "\n",
    "CNNs are particularly powerful for tasks involving images. This is because they can pick up on patterns like edges, shapes, and textures to help the network understand the image as a whole.\n",
    "\n",
    "### Layers in CNN:\n",
    "\n",
    "1. Convolutional Layers: These are the core building blocks of a CNN. They apply different filters (like edge detection, blur, etc.) to the input (like an image) and create feature maps. Each filter helps in identifying different features in the image.\n",
    "\n",
    "2. Pooling Layers: These layers follow convolutional layers and are used to reduce the spatial size (width and height) of the feature maps. This reduces the amount of computation and parameters, helping to prevent overfitting. Common pooling methods include max pooling (taking the maximum value in a window) and average pooling.\n",
    "\n",
    "3. Flattening: After several convolutional and pooling layers, the high-level reasoning in the neural network is done via fully connected layers. However, to connect a 3D volume to a fully connected layer, we must first flatten it into a 1D vector.\n",
    "\n",
    "4. Fully Connected Layers: These layers connect every neuron from one layer to every neuron in the next layer, similar to regular neural networks. It's where the network combines all the learned features from the previous layers to determine the final output, such as the classification of the image.\n",
    "\n",
    "5. Output Layer: This layer gives the final output. For example, in a classification task, it would give the probabilities of the image being each class.\n",
    "\n",
    "### How it Learns:\n",
    "\n",
    "The CNN learns through a process called backpropagation. In simple terms, it involves:\n",
    "\n",
    "* Feeding the network with input data (like images).\n",
    "* Letting the network make a prediction.\n",
    "* Comparing the prediction with the actual label.\n",
    "* Adjusting the weights of the filters (the parameters of the model) to reduce errors in predictions.\n",
    "  \n",
    "### Why CNNs are Great:\n",
    "\n",
    "CNNs are excellent for image processing because they can capture spatial hierarchies in the data. For example, the first layer might recognize edges, the next layer could recognize textures, then patterns, and finally specific objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(30, (3,3), activation = 'relu', input_shape = (28, 28, 1)),\n",
    "    keras.layers.MaxPool2D((2,2)),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Convolutional Neural Network Model\n",
    "\n",
    "1. `model = keras.Sequential([...])`:\n",
    "    - This line of code initiates the creation of a CNN model.\n",
    "    - `keras.Sequential` is used here again to define a sequential layer arrangement for the neural network.\n",
    "\n",
    "2. Inside the `Sequential` constructor, the layers of the CNN are defined:\n",
    "    - `keras.layers.Conv2D(30, (3,3), activation='relu', input_shape=(28, 28, 1))`: This is the first layer, a convolutional layer.\n",
    "        - It has 30 filters (kernels), each of size 3x3.\n",
    "        - `activation='relu'` specifies the use of the ReLU (Rectified Linear Unit) activation function.\n",
    "        - `input_shape=(28, 28, 1)` denotes the expected input shape for the layer, which is 28x28 pixels with 1 color channel (grayscale).\n",
    "    - `keras.layers.MaxPool2D((2,2))`: This is a max pooling layer.\n",
    "        - It reduces the spatial dimensions (height and width) of the input volume for the next convolutional layer.\n",
    "        - `(2,2)` specifies the size of the window over which to take the maximum.\n",
    "    - `keras.layers.Flatten()`: This layer flattens the 2D arrays to a 1D array to be fed into the dense layers.\n",
    "        - After convolutional and pooling layers, it's common to flatten the features to pass them through dense layers for classification.\n",
    "    - `keras.layers.Dense(100, activation='relu')`: A dense layer with 100 neurons and ReLU activation function.\n",
    "    - `keras.layers.Dense(10, activation='sigmoid')`: The output layer of the network.\n",
    "        - It has 10 neurons since there are 10 classes to predict.\n",
    "        - `activation='sigmoid'` is used here. However, for multi-class classification tasks, `softmax` is generally more appropriate. The use of `sigmoid` might be a specific design choice or an oversight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.4094 - accuracy: 0.8542\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2705 - accuracy: 0.9017\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.2255 - accuracy: 0.9174\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1933 - accuracy: 0.9299\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1680 - accuracy: 0.9388\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1442 - accuracy: 0.9469\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1243 - accuracy: 0.9538\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1066 - accuracy: 0.9609\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0914 - accuracy: 0.9663\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0786 - accuracy: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25c48a8e310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the CNN Model\n",
    "\n",
    "1. `model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])`:\n",
    "    - This line compiles the model, which is a necessary step before training.\n",
    "    - `optimizer='adam'`: This sets the Adam optimizer for the training process. Adam is a popular choice as it combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "    - `loss='sparse_categorical_crossentropy'`: Specifies the loss function to be used. For multi-class classification problems like this (with integer labels), sparse categorical crossentropy is typically used.\n",
    "    - `metrics=['accuracy']`: Indicates that the model should track accuracy during training and evaluation. This metric is used to evaluate the performance of the model, with higher accuracy indicating better performance.\n",
    "\n",
    "### Train the CNN Model\n",
    "\n",
    "2. `model.fit(x_train, y_train, epochs=100)`:\n",
    "    - This line initiates the training of the model using the training data.\n",
    "    - `model.fit` is the method used for training. \n",
    "    - `x_train` and `y_train` are the training dataset and corresponding labels.\n",
    "    - `epochs=100` specifies that the training process should run for 100 epochs. An epoch is a full iteration over the entire training dataset. The choice of 100 epochs means the model will have ample opportunity to learn from the data, though care must be taken to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3088 - accuracy: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30879437923431396, 0.9154000282287598]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
